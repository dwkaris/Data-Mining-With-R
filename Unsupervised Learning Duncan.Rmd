---
title: "Unsupervised Learning"
output:
  pdf_document: default
  pdf: default
---

#WEEK 12 INDEPENDENT PROJECT

##DEFINING THE ANALYTIC QUESTION QUESTION


###a). Specifying the analytic question

As Data Scientists, we have been tasked with building a model by the brand’s Sales and Marketing team at Kira Plastinina that understands their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.


###b). Defining the metrics of success

Our success will be determined by building a model with the best metrics scores ie the best accuracy score and Confusion Matrix classification  without overfiting or underfiting.


###c) Understanding the Context

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year.

More specifically, they would like to learn the characteristics of customer groups. Thus, we have been tasked with Performing clustering stating insights drawn from our analysis and visualizations and upon implementation, provide comparisons between the unsupervised approaches  K-Means clustering vs Hierarchical clustering highlighting the strengths and limitations of each approach in the context of our analysis. Our findings will help inform the team in formulating the marketing and sales strategies of the brand. 


###d). Recording the Experimental Design

Below is the expected workflow of our Analysis

1.   Data Cleaning and Preparation

*   Load libraries and dataset
*   Deal with missing values
  
2.   Perform EDA

*   Univariate Analysis
*   Bivariate Analysis

3.  Splitting dataset into testing and training data

4.  Carry out classification while providing description of how each model differsand their limitation 

*   K-Mean Clustering
*   Hierarchical Clustering

5.  Make predictions using the different Models
6.  Access accuracy of different models
7.  Make conclusions & Challenge the solution


###e) Data relevance/ Appropriateness of Data

Our study aims at creating the best model that will help our client learn the characteristics of their customer groups. Below is the link and dataset description:

*   The dataset consists of 10 numerical and 8 categorical attributes. The 'Revenue' attribute can be used as the class label.

*   "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another. 

*   The "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 

*    value of the "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.

*   The value of the "Exit Rate" feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.

*   The "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. 

*   The "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. 

*   The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


## Importing Libraries

```{r}
install.packages("tidyverse")
install.packages("corrplot")
install.packages("GGally")
install.packages("cluster")
install.packages("pander")
install.packages("Hmisc")


suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(cluster))

library(dplyr)
library(tidyr)
library(ggplot2)
library(tibble)
library(pander)
library(Hmisc)


```

## Load the Dataset

```{r}

# Load Preview and top 

dt <- as.tibble(read_csv("C:\\Users\\user\\Downloads\\online_shoppers_intention.csv"))
head(dt)

```
```{r}

# Load Preview and bottom

tail(dt)
```


## Checking the data

Check shape of our dataset

```{r}
dim(dt)
```
our dataset has 12330 rows and 18 columns

Check Number of Columns Present
```{r}

colnames(dt)
```
our dataset has 18 columns

shows the data types of the data frame

```{r}
glimpse(dt)

```

Our dataset has 14 numerical variables 2 categorical variable and 2 boolean variable.


Check summary of our dataset

```{r}

summ <- data.frame(summary(df))
pander(summ)


```
This give of every variable there mean, max,min, datatype and quantiles


## External Data Source Validation

Making sure your data matches something outside of the dataset is very important. It allows you to ensure that the measurements are roughly in line with what they should be and it serves as a check on what other things might be wrong in your dataset. External validation can often be as simple as checking your data against a single number, as we will do here.




## Tidying the Dataset

We start by checking for null values in our dataset

```{r}
# Check for missing values

colSums(is.na(dt))

```

since null values are very few we will drop them
```{r}
dt_complete<-na.omit(dt)
colSums(is.na(dt_complete))
dim(dt_complete)
```


Convert boolean column to numerical 

```{r}

cols.num <- c("Revenue", "Weekend" )
dt_complete[cols.num] <- sapply(dt_complete[cols.num],as.numeric)
sapply(dt_complete, class)


```

check for duplicated values
```{r}
#  check for duplicated values

duplicated_rows <- dt_complete[duplicated(dt_complete),]


dt_complete <- unique(dt_complete)
dim(dt_complete)
```
Our dataset had 117 duplicated columns hence we have droped them


Select only the numerical columns hence droping the redudant character columns

```{r}

dt_num <- dplyr::select_if(dt_complete, is.numeric)

head(dt_num)

```


```{r}
colnames(dt_num)
```
We will now havw 16 columns for analysis


Set our outcome to categorical

```{r}
dt_num$Revenue <- factor(dt_num$Revenue)
```


## Explanatory data analysis

### Univariate analysis And Bivariate Distribution


Visualize our target

```{r, fig.height= 5, fig.width= 10}

# Count our clicks

ggplot(dt_num) + geom_bar(aes(x = Revenue),fill = 'orange')

```
Our dataset has False Than True values in the Revenue variable



A display of our categorical variables vs Revenue

```{r Fig1, fig.height=5, fig.width= 10}
par(mfrow=c(8,1))
ggplot(dt_num , mapping = aes(x = Administrative, color = Revenue )) +
         geom_freqpoly() + labs(title = "Administrative Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Informational, color = Revenue )) +
         geom_freqpoly() + labs(title = "Informational Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = SpecialDay, color = Revenue )) +
         geom_freqpoly() + labs(title = "SpecialDay Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = OperatingSystems, color = Revenue )) +
         geom_freqpoly() + labs(title = "OperatingSystems Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Browser, color = Revenue )) +
         geom_freqpoly() + labs(title = "Browser Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Region, color = Revenue )) +
         geom_freqpoly() + labs(title = "Region Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = TrafficType, color = Revenue )) +
         geom_freqpoly() + labs(title = "TrafficType Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Weekend, color = Revenue )) +
         geom_freqpoly() + labs(title = "Weekend Distribution by Revenue")
```
Administrative  by Revenue 
Both categories of revenue have there peaks at zero with False having more imputes than True

Informational Distribution by Revenue
Both categories of revenue have there peaks at zero with False having more imputes than True

SpecialDay Distribution by Revenue
Both categories of revenue have there peaks at zero when its not a special day with False having more imputes than True.

OperatingSystems Distribution by Revenue
Both categories of revenue have there peaks at Operating system 2, follwed by 3,1, 4 respectively with False having more imputes than True.

Browser Distribution by Revenue
Both categories of revenue have there peaks at Browser distribution 2, with False having more imputes than True.

Weekend Distribution by Revenue
Both categories of revenue have there peaks when its not a weekend, with False having more imputes than True.



A display of our continuous variables vs Revenue

```{r Fig2}

A <- ggplot(dt_num , mapping = aes(x = Administrative_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "Administrative_Duration Distribution by  Revenue")
A + theme_bw()

I <- ggplot(dt_num , mapping = aes(x = Informational_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "Informational_Duration Distribution by  Revenue")
I + theme_bw()

R <- ggplot(dt_num , mapping = aes(x = ProductRelated ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ProductRelated Distribution by  Revenue")
R + theme_bw()

P <- ggplot(dt_num , mapping = aes(x = ProductRelated_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ProductRelated_Duration Distribution by  Revenue")
P + theme_bw()

B <- ggplot(dt_num , mapping = aes(x = BounceRates,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "Bounce Rates Distribution by  Revenue")
B + theme_bw()

E <- ggplot(dt_num , mapping = aes(x = ExitRates ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ExitRates Distribution by  Revenue")
E + theme_bw()

V <- ggplot(dt_num , mapping = aes(x = PageValues ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "PageValues Distribution by  Revenue")
V + theme_bw()


```
Administrative_Duration Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks at zero with False having more imputes than True.

Informational_Duration Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks at zero with False having more imputes than True

ProductRelated Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks close to zero with False having more imputes than True

ProductRelated_Duration Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks close to zero with False having more imputes than True


Bounce Rates Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks close to zero with False having more imputes than True

ExitRates Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks close to zero with False having more imputes than True

PageValues Distribution by  Revenue
Its skewed towards the left with both categories of revenue have there peaks close to zero with False having more imputes than True



find correlation between columns

```{r}
#use Hmisc package

res <- rcorr(as.matrix(dt_num))
corr <- data.frame(res$r)
corr

# get relationship between Revenue and other variables


```


Create a correlation plot

```{r, fig.height=10, fig.width= 10}

corrplot(res$r,  order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

```{r}

rev <- data.frame(corr$Revenue)

rownames(rev) <- colnames(corr)

rev
```
Of all our variables PageValues, ProductRelated, and Administrative have the closest relationship to our target variable.



##Implimenting the Solution


###Feature selection 


We will only pick variables with a positive correlation as this are the ones that will improve our model


```{r}
column <- c('Administrative', 'Administrative_Duration',"Informational", "Informational_Duration",
            "ProductRelated", "ProductRelated_Duration", "PageValues", "Browser", "Weekend")
df <- dt_num[column]
head(df)
```




###Modeling Data Using K-Means Clustering

We will assign a variable to our categorical variable

```{r}
target <- dt_num$Revenue

```



scale our Features dataframe

```{r}
# We define a normal function which will normalize the set of values according to its minimum value and maximum value.
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
normal(1:9)
df_scaled <- as.data.frame(lapply(df, normal))
summary(df_scaled)

```

Applying the K-means clustering algorithm with no. of centroids(k)=3

```{r}
result<- kmeans(df_scaled,3) 

```

Previewing the no. of records in each cluster

```{r}
result$size
```

Getting the value of cluster center datapoint value(3 centers for k=3)

```{r}
result$centers 
```

Getting the cluster vector that shows the cluster where each record falls

```{r}
head(result$cluster)
tail(result$cluster)
```

Visualizing diffrent variables the  clustering results

```{r}
par(mfrow = c(1,2), mar = c(5,4,2,2))

plot(df_scaled[,1:2], col = result$cluster) 
plot(df_scaled[,3:4], col = result$cluster) 
```

Verifying the results of clustering

```{r}
table(result$cluster, target)
```
The output above shows the distribution of diffrent clusters across diffrent variables



HIERACHIAL CLUSTERING

Build distance matrix

```{r}

dist_mat <- dist(df_scaled, method = 'euclidean')
```


Decide linkage method to use

```{r}
hclust_avg <- hclust(dist_mat, method = "average")
plot(hclust_avg)

# Create a dndogram
```

Use cut trees function to cut the tree

```{r}
cut_avg <- cutree(hclust_avg, k=2)
```


Visualize dendogram using rectangular compactments

```{r}
plot(hclust_avg)

rect.hclust(hclust_avg, k=3, border = 2:6)

abline(h=3, col = 'red')
```


```{r}
install.packages("dendextend")
suppressPackageStartupMessages(library(dendextend))
```

Visualize dendogram trees

```{r}
avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, k= 3)
plot(avg_col_dend)
```
Append dendogram in original dataframe

```{r}

suppressPackageStartupMessages(library(dplyr))
df_cf <- mutate(df, cluster = cut_avg)
count(df_cf, cluster)

```



## Challenging the solution

From our analysis K-means clustering out performed the Hierarchical clustrering as it was able to cluster our features to 3 distinct groups as we had defined in our model 

Advantages of K- means Clustering

* Easy to implement 
* With a large number of variables, K-Means may be computationally faster than hierarchical clustering (if K is small). 
* k-Means may produce Hghter clusters than hierarchical clustering 
* An instance can change cluster (move to another cluster) when the centroids are recomputed. 


Disadvantages of K- means Clustering

* Difficult to predict the number of clusters (K-Value) 
* Initial seeds have a strong impact on the final results 
* The order of the data has an impact on the final results 
* Sensitive to scale: rescaling your datasets (normalization or standardization) will completely change results. 

Advantages of Hierarchical Clustering

* We do not need to specify the number of clusters required for the algorithm.
Hierarchical clustering outputs a hierarchy, ie a structure that is more informative than the unstructured  

* set of flat clusters returned by k-means.

* It is also easy to implement.


Disadvantages  of Hierarchical Clustering

* There is no mathematical objective for Hierarchical clustering.

* All the approaches to calculate the similarity between clusters has its own disadvantages.

* High space and time complexity for Hierarchical clustering. Hence this clustering algorithm cannot be used when we have huge data.



##Follow up questions

At this point, we can refine our question or collect new data, all in an iterative process to get at the truth.


a). Did we have the right data?

Yes
 
because using the data we were able to conduct an analysis on our analytic question

b). Do we need other data to answer our question?

no.

The data provided was sufficient

c). Did we have the right question?

yes

We were able to make conclusion from our data

