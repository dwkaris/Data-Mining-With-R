---
title: "Unsupervised Learning"
output:
  pdf_document: default
  pdf: default
---

#WEEK 12 INDEPENDENT PROJECT

##DEFINING THE ANALYTIC QUESTION QUESTION


###a). Specifying the analytic question

As Data Scientists, we have been tasked with building a model by the brand’s Sales and Marketing team at Kira Plastinina that understands their customer’s behavior from data that they have collected over the past year. More specifically, they would like to learn the characteristics of customer groups.


###b). Defining the metrics of success

Our success will be determined by building a model with the best metrics scores ie the best accuracy score and Confusion Matrix classification  without overfiting or underfiting.


###c) Understanding the Context

Kira Plastinina is a Russian brand that is sold through a defunct chain of retail stores in Russia, Ukraine, Kazakhstan, Belarus, China, Philippines, and Armenia. The brand’s Sales and Marketing team would like to understand their customer’s behavior from data that they have collected over the past year.

More specifically, they would like to learn the characteristics of customer groups. Thus, we have been tasked with Performing clustering stating insights drawn from our analysis and visualizations and upon implementation, provide comparisons between the unsupervised approaches  K-Means clustering vs Hierarchical clustering highlighting the strengths and limitations of each approach in the context of our analysis. Our findings will help inform the team in formulating the marketing and sales strategies of the brand. 


###d). Recording the Experimental Design

Below is the expected workflow of our Analysis

1.   Data Cleaning and Preparation

*   Load libraries and dataset
*   Deal with missing values
  
2.   Perform EDA

*   Univariate Analysis
*   Bivariate Analysis

3.  Splitting dataset into testing and training data

4.  Carry out classification while providing description of how each model differsand their limitation 

*   K-Mean Clustering
*   Hierarchical Clustering

5.  Make predictions using the different Models
6.  Access accuracy of different models
7.  Make conclusions & Challenge the solution


###e) Data relevance/ Appropriateness of Data

Our study aims at creating the best model that will help our client learn the characteristics of their customer groups. Below is the link and dataset description:

*   The dataset consists of 10 numerical and 8 categorical attributes. The 'Revenue' attribute can be used as the class label.

*   "Administrative", "Administrative Duration", "Informational", "Informational Duration", "Product Related" and "Product Related Duration" represents the number of different types of pages visited by the visitor in that session and total time spent in each of these page categories. The values of these features are derived from the URL information of the pages visited by the user and updated in real-time when a user takes an action, e.g. moving from one page to another. 

*   The "Bounce Rate", "Exit Rate" and "Page Value" features represent the metrics measured by "Google Analytics" for each page in the e-commerce site. 

*    value of the "Bounce Rate" feature for a web page refers to the percentage of visitors who enter the site from that page and then leave ("bounce") without triggering any other requests to the analytics server during that session.

*   The value of the "Exit Rate" feature for a specific web page is calculated as for all pageviews to the page, the percentage that was the last in the session.

*   The "Page Value" feature represents the average value for a web page that a user visited before completing an e-commerce transaction. 

*   The "Special Day" feature indicates the closeness of the site visiting time to a specific special day (e.g. Mother’s Day, Valentine's Day) in which the sessions are more likely to be finalized with the transaction. The value of this attribute is determined by considering the dynamics of e-commerce such as the duration between the order date and delivery date. For example, for Valentina’s day, this value takes a nonzero value between February 2 and February 12, zero before and after this date unless it is close to another special day, and its maximum value of 1 on February 8. 

*   The dataset also includes the operating system, browser, region, traffic type, visitor type as returning or new visitor, a Boolean value indicating whether the date of the visit is weekend, and month of the year.


## Importing Libraries

```{r}
install.packages("tidyverse")
install.packages("corrplot")
install.packages("GGally")
install.packages("cluster")
install.packages("pander")
install.packages("Hmisc")


suppressPackageStartupMessages(library(tidyverse))
suppressPackageStartupMessages(library(corrplot))
suppressPackageStartupMessages(library(GGally))
suppressPackageStartupMessages(library(cluster))

library(dplyr)
library(tidyr)
library(ggplot2)
library(tibble)
library(pander)
library(Hmisc)


```

## Load the Dataset

```{r}

# Load Preview and top 

dt <- as.tibble(read_csv("C:\\Users\\user\\Downloads\\online_shoppers_intention.csv"))
head(dt)

```
```{r}

# Load Preview and bottom

tail(dt)
```


## Checking the data

Check shape of our dataset

```{r}
dim(dt)
```
our dataset has 12330 rows and 18 columns

Check Number of Columns Present
```{r}

colnames(dt)
```

shows the data types of the data frame

```{r}
glimpse(dt)

```

Our dataset has 14 numerical variables 2 categorical variable and 2 boolean variable.

Check summary of our dataset

```{r}

summ <- data.frame(summary(df))
pander(summ)


```
This give of every variable there mean, max,min, datatype and quantiles

## External Data Source Validation

Making sure your data matches something outside of the dataset is very important. It allows you to ensure that the measurements are roughly in line with what they should be and it serves as a check on what other things might be wrong in your dataset. External validation can often be as simple as checking your data against a single number, as we will do here.


## Tidying the Dataset

We start by checking for null values in our dataset

```{r}
# Check for missing values

colSums(is.na(dt))

```

since null values are very few we will drop them
```{r}
dt_complete<-na.omit(dt)
colSums(is.na(dt_complete))
dim(dt_complete)
```

Outlier detection and dealing with outliers

Convert categorical column

```{r}

cols.num <- c("Revenue", "Weekend" )
dt_complete[cols.num] <- sapply(dt_complete[cols.num],as.numeric)
sapply(dt_complete, class)


```
check for duplicated values
```{r}
#  check for duplicated values

duplicated_rows <- dt_complete[duplicated(dt_complete),]


dt_complete <- unique(dt_complete)
dim(dt_complete)
```


Select only the numerical columns

```{r}

dt_num <- dplyr::select_if(dt_complete, is.numeric)

head(dt_num)

```


```{r}
colnames(dt_num)
```


Set our outcome to categorical

```{r}
dt_num$Revenue <- factor(dt_num$Revenue)
```


## Explanatory data analysis

### Univariate analysis And Bivariate Distribution


A display of our categorical variables vs Revenue

```{r Fig1, fig.height=5, fig.width= 10}
par(mfrow=c(8,1))
ggplot(dt_num , mapping = aes(x = Administrative, color = Revenue )) +
         geom_freqpoly() + labs(title = "Administrative Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Informational, color = Revenue )) +
         geom_freqpoly() + labs(title = "Informational Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = SpecialDay, color = Revenue )) +
         geom_freqpoly() + labs(title = "SpecialDay Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = OperatingSystems, color = Revenue )) +
         geom_freqpoly() + labs(title = "OperatingSystems Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Browser, color = Revenue )) +
         geom_freqpoly() + labs(title = "Browser Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Region, color = Revenue )) +
         geom_freqpoly() + labs(title = "Region Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = TrafficType, color = Revenue )) +
         geom_freqpoly() + labs(title = "TrafficType Distribution by Revenue")

ggplot(dt_num , mapping = aes(x = Weekend, color = Revenue )) +
         geom_freqpoly() + labs(title = "Weekend Distribution by Revenue")
```

A display of our continuous variables vs Revenue

```{r Fig2}

A <- ggplot(dt_num , mapping = aes(x = Administrative_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "Administrative_Duration Distribution by  Revenue")
A + theme_bw()

I <- ggplot(dt_num , mapping = aes(x = Informational_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "Informational_Duration Distribution by  Revenue")
I + theme_bw()

R <- ggplot(dt_num , mapping = aes(x = ProductRelated ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ProductRelated Distribution by  Revenue")
R + theme_bw()

P <- ggplot(dt_num , mapping = aes(x = ProductRelated_Duration,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ProductRelated_Duration Distribution by  Revenue")
P + theme_bw()

B <- ggplot(dt_num , mapping = aes(x = BounceRates,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "BounceRates Distribution by  Revenue")
B + theme_bw()

E <- ggplot(dt_num , mapping = aes(x = ExitRates ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "ExitRates Distribution by  Revenue")
E + theme_bw()

V <- ggplot(dt_num , mapping = aes(x = PageValues ,fill = Revenue , color = Revenue )) +
         geom_histogram() + labs(title = "PageValues Distribution by  Revenue")
V + theme_bw()


```

find correlation between columns

```{r}
#use Hmisc package

res <- rcorr(as.matrix(dt_num))
corr <- data.frame(res$r)
corr

# get relationship between Revenue and other variables


```

Create a correlation plot

```{r, fig.height=10, fig.width= 10}

corrplot(res$r,  order = "hclust", 
         tl.col = "black", tl.srt = 45)
```

Feature selection 


```{r}

rev <- data.frame(corr$Revenue)

rownames(rev) <- colnames(corr)

rev
```

We will only pick variables with a positive correlation as this are the ones that will improve our model


```{r}
column <- c('Administrative', 'Administrative_Duration',"Informational", "Informational_Duration",
            "ProductRelated", "ProductRelated_Duration", "PageValues", "Browser", "Weekend")
df <- dt_num[column]
head(df)
```



##Implimenting the Solution

###Modeling Data Using K-Means Clustering

scale our dataframe

```{r}
# We define a normal function which will normalize the set of values according to its minimum value and maximum value.
normal <- function(x) (
  return( ((x - min(x)) /(max(x)-min(x))) )
)
normal(1:9)
df_scaled <- as.data.frame(lapply(df, normal))
summary(df_scaled)

```
Kmeans with 3 centers
```{r}


set.seed(123)
final <- kmeans(df_scaled, centers = 3, nstart = 25)
print(final)
```
Kmeans with 4 centers

```{r}
set.seed(123)
final4 <- kmeans(df_scaled, centers = 4, nstart = 25)
print(final4)
```
```{r}
set.seed(123)
final7 <- kmeans(df_scaled, centers = 7, nstart = 25)
print(final7)
```

```{r}
df_scaled  %>% mutate(cluster = final$cluster) %>% group_by(cluster) %>% summarize_all("median")
```


HIERACHIAL CLUSTERING

Build distance matrix

```{r}

dist_mat <- dist(df_scaled, method = 'euclidean')
```


Decide linkage method to use

```{r}
hclust_avg <- hclust(dist_mat, method = "average")
plot(hclust_avg)

# Create a dndogram
```

Use cut trees function to cut the tree

```{r}
cut_avg <- cutree(hclust_avg, k=3)
```


Visualize dendogram using rectangular compactments

```{r}
plot(hclust_avg)

rect.hclust(hclust_avg, k=3, border = 2:6)

abline(h=3, col = 'red')
```


```{r}
install.packages("dendextend")
suppressPackageStartupMessages(library(dendextend))
```

Visualize dendogram trees

```{r}
avg_dend_obj <- as.dendrogram(hclust_avg)
avg_col_dend <- color_branches(avg_dend_obj, k= 4)
plot(avg_col_dend)
```
Append dendogram in original dataframe

```{r}
suppressPackageStartupMessages(library(dplyr))
df_cf <- mutate(df, cluster = cut_avg)
count(df_cf, cluster)
df_cf
```
```

## Challenging the solution





##Follow up questions


At this point, we can refine our question or collect new data, all in an iterative process to get at the truth.


a). Did we have the right data?

Yes
 
because using the data we were able to conduct an analysis on our analytic question

b). Do we need other data to answer our question?

no.

The data provided was sufficient

c). Did we have the right question?

yes

We were able to make conclusion from our data